# Automated Investigation System

This directory contains the automated investigation system that runs daily via GitHub Actions to analyze the Epstein Files dataset.

## Components

### `database.py`
SQLite database management for the investigation system.

**Tables:**
- `documents`: Tracks all ingested files with metadata and content hashes
- `entities`: Stores extracted entities (people, organizations, locations)
- `entity_connections`: Maps relationships between entities found in documents
- `leads`: Investigation leads generated by the decision logic
- `suspicious_docs`: Documents flagged for suspicious patterns
- `statistics`: Daily statistics tracking

### `ingest.py`
File ingestion and entity extraction module.

**Features:**
- Scans `data/processed/files/` for new text files
- Extracts metadata from filenames (category, source)
- Calculates SHA256 hashes to prevent duplicate ingestion
- Basic entity extraction using regex patterns
- Idempotent - can be run multiple times safely

### `analytics.py`
Advanced analytics and pattern detection.

**Features:**
- **Benford's Law Detection**: Uses chi-squared test to detect statistical anomalies in document numbers
- **Bridge Finding**: Identifies highly connected entities (entities appearing with many others)
- Entity relationship graph generation
- Automatic document flagging based on statistical analysis

**Benford's Law**: Numbers in natural datasets should follow a specific distribution for first digits. Fabricated data often violates this law.

### `orchestrator.py`
Main coordination layer that runs the complete investigation pipeline.

**Pipeline Steps:**
1. Load/initialize the investigation database
2. Run file ingestion to add new documents
3. Run analytics to extract entities and detect patterns
4. Apply decision logic:
   - Generate leads for new bridge entities (3+ connections)
   - Flag documents with Benford's Law violations
5. Generate comprehensive daily briefing report
6. Save results to `investigation.db` and `DAILY_BRIEFING.md`

## Usage

### Run Manually
```bash
# From repository root
python3 src/orchestrator.py
```

### Automated Execution
The system runs automatically every day at 3 AM UTC via GitHub Actions (`.github/workflows/daily_investigation.yml`).

Results are committed back to the repository:
- `investigation.db` - Updated database with new findings
- `DAILY_BRIEFING.md` - Daily investigation report

## Decision Logic

### Lead Generation
- **Bridge Entities**: Entities with 3+ connections to other entities are automatically added as leads
- **Priority Levels**:
  - HIGH: 10+ connections
  - MEDIUM: 3-9 connections
  
### Suspicious Document Detection
- **Benford's Law Violations**: Documents with 50+ numbers showing chi-squared > 15.507
- **Severity**: MEDIUM (requires manual review)

## Output Format

### Daily Briefing (`DAILY_BRIEFING.md`)
- Executive summary with key metrics
- New leads requiring investigation
- Suspicious documents flagged
- Top bridge entities with connection details
- Analysis and database statistics

## Requirements

- Python 3.7+
- Standard library only (no external dependencies)
- tesseract-ocr (for GitHub Actions, though not currently used)

## Database Location

The `investigation.db` file is stored in the repository root and tracked in git to maintain investigation state across runs.
